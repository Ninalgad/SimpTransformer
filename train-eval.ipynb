{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train-eval.ipynb","provenance":[],"collapsed_sections":["TyDWXfg__PsA","OLhr_lWpJSJ9","HW0FuN2E_Hk-","3P6PnUdRJ3Mj"],"authorship_tag":"ABX9TyM2p9UwyKJPVIUEoLfNbLhX"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ll0heREn3WNG"},"source":["We use Colab for free GPU training. For accelerated training make sure you are using the GPU harware accelerator: Runtime ->  Change runtime type \\\\\n","To check you are running a GPU runtime, run the following:"]},{"cell_type":"code","metadata":{"id":"6Lc5aKSH4FRR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616534957549,"user_tz":240,"elapsed":502,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"0d329810-8285-4256-d0ae-0c51441213bb"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find(\"failed\") >= 0:\n","  print(\"Not connected to a gpu runtime\")\n","else:\n","  print(\"Connected to a gpu runtime:\")\n","  print(gpu_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Connected to a gpu runtime:\n","Tue Mar 23 21:29:17 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4z29FqHf5e8T"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"y_mJzqdS_OZZ","executionInfo":{"status":"ok","timestamp":1616542602866,"user_tz":240,"elapsed":244,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["import numpy as np\n","import tensorflow as tf\n","from tqdm.notebook import tqdm\n","import time\n","from IPython import display\n","import pickle\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","\n","params = AttrDict({\n","\n","    # dataset creation parameters\n","    'dataset_path': '',\n","    'dataset_size': 3000,\n","    'num_parallel_processes': 15,\n","    'env_name': 'char_sp',\n","    'int_base': 10,\n","    'balanced': False,\n","    'positive': True,\n","    'precision': 10,\n","    'n_variables': 2,\n","    'n_coefficients': 0,\n","    'leaf_probs': '0.75,0,0.25,0',\n","    'max_len': 510,\n","    'max_int': 5,\n","    'max_ops': 15,\n","    'max_ops_G': 15,\n","    'clean_prefix_expr': True,\n","    'rewrite_functions': '',\n","    'tasks': 'prim_fwd',\n","\n","    # used operations, followed by genreation weight\n","    'operators': 'add:1,sub:1,mul:1,pow2:1,pow3:1', \n","    #'operators': 'add:10,sub:3,mul:10,div:5,sqrt:4,pow2:4,pow3:2,pow4:1,pow5:1,ln:4,exp:4,sin:4,cos:4,tan:4,asin:1,acos:1,atan:1,sinh:1,cosh:1,tanh:1,asinh:1,acosh:1,atanh:1',\n","\n","    # model hyperparameters\n","    'num_layers': 4,\n","    'd_model': 512,\n","    'dff': 512,\n","    'num_heads': 8,\n","    'dropout_rate': 0.1,\n","\n","    # model training hyperparameters\n","    'learning_rate': 1e-4,\n","    'batch_size': 32,\n","    'model_name': 'lime',\n","    'epochs': 300,\n","    'model_path': 'model.h5'\n","    })"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-akHx_q5fAmU"},"source":["## Equation Generation Environment Class\n","\n","This cell contains the utility functions for generaiting random expressions. This is modified version of the code used for [\tarXiv:1912.01412](https://arxiv.org/abs/1912.01412) for generating random integrals and is built with Sympy (see [SymbolicMathematics](https://github.com/facebookresearch/SymbolicMathematics) ). "]},{"cell_type":"code","metadata":{"id":"MNoGKEUBLw1t","executionInfo":{"status":"ok","timestamp":1616544462944,"user_tz":240,"elapsed":3853,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["import sympy as sp\n","from sympy.parsing.sympy_parser import parse_expr\n","from sympy.core.cache import clear_cache\n","from sympy.integrals.risch import NonElementaryIntegral\n","from sympy.calculus.util import AccumBounds\n","import os\n","import io\n","import re\n","import sys\n","import math\n","import itertools\n","from collections import OrderedDict\n","import numpy as np\n","import numexpr as ne\n","\n","\n","SPECIAL_WORDS = ['<s>', '</s>', '<pad>', '(', ')']\n","SPECIAL_WORDS = SPECIAL_WORDS + [f'<SPECIAL_{i}>' for i in range(len(SPECIAL_WORDS), 10)]\n","\n","\n","INTEGRAL_FUNC = {sp.erf, sp.erfc, sp.erfi, sp.erfinv, sp.erfcinv, sp.expint, sp.Ei, sp.li, sp.Li, sp.Si, sp.Ci, sp.Shi, sp.Chi, sp.fresnelc, sp.fresnels}\n","EXP_OPERATORS = {'exp', 'sinh', 'cosh'}\n","EVAL_SYMBOLS = {'x', 'y', 'z', 'a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9'}\n","EVAL_VALUES = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 2.1, 3.1]\n","EVAL_VALUES = EVAL_VALUES + [-x for x in EVAL_VALUES]\n","\n","TEST_ZERO_VALUES = [0.1, 0.9, 1.1, 1.9]\n","TEST_ZERO_VALUES = [-x for x in TEST_ZERO_VALUES] + TEST_ZERO_VALUES\n","ZERO_THRESHOLD = 1e-13\n","\n","\n","class ValueErrorExpression(Exception):\n","    pass\n","\n","\n","class UnknownSymPyOperator(Exception):\n","    pass\n","\n","\n","class InvalidPrefixExpression(Exception):\n","\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __str__(self):\n","        return repr(self.data)\n","\n","\n","def count_nested_exp(s):\n","    \"\"\"\n","    Return the maximum number of nested exponential functions in an infix expression.\n","    \"\"\"\n","    stack = []\n","    count = 0\n","    max_count = 0\n","    for v in re.findall('[+-/*//()]|[a-zA-Z0-9]+', s):\n","        if v == '(':\n","            stack.append(v)\n","        elif v == ')':\n","            while True:\n","                x = stack.pop()\n","                if x in EXP_OPERATORS:\n","                    count -= 1\n","                if x == '(':\n","                    break\n","        else:\n","            stack.append(v)\n","            if v in EXP_OPERATORS:\n","                count += 1\n","                max_count = max(max_count, count)\n","    assert len(stack) == 0\n","    return max_count\n","\n","\n","def is_valid_expr(s):\n","    \"\"\"\n","    Check that we are able to evaluate an expression (and that it will not blow in SymPy evaluation).\n","    \"\"\"\n","    s = s.replace('Derivative(f(x),x)', '1')\n","    s = s.replace('Derivative(1,x)', '1')\n","    s = s.replace('(E)', '(exp(1))')\n","    s = s.replace('(I)', '(1)')\n","    s = s.replace('(pi)', '(1)')\n","    s = re.sub(r'(?<![a-z])(f|g|h|Abs|sign|ln|sin|cos|tan|sec|csc|cot|asin|acos|atan|asec|acsc|acot|tanh|sech|csch|coth|asinh|acosh|atanh|asech|acoth|acsch)\\(', '(', s)\n","    count = count_nested_exp(s)\n","    if count >= 4:\n","        return False\n","    for v in EVAL_VALUES:\n","        try:\n","            local_dict = {s: (v + 1e-4 * i) for i, s in enumerate(EVAL_SYMBOLS)}\n","            value = ne.evaluate(s, local_dict=local_dict).item()\n","            if not (math.isnan(value) or math.isinf(value)):\n","                return True\n","        except (FloatingPointError, ZeroDivisionError, TypeError, MemoryError):\n","            continue\n","    return False\n","\n","\n","def eval_test_zero(eq):\n","    \"\"\"\n","    Evaluate an equation by replacing all its free symbols with random values.\n","    \"\"\"\n","    variables = eq.free_symbols\n","    assert len(variables) <= 3\n","    outputs = []\n","    for values in itertools.product(*[TEST_ZERO_VALUES for _ in range(len(variables))]):\n","        _eq = eq.subs(zip(variables, values)).doit()\n","        outputs.append(float(sp.Abs(_eq.evalf())))\n","    return outputs\n","\n","\n","def has_inf_nan(*args):\n","    \"\"\"\n","    Detect whether some expressions contain a NaN / Infinity symbol.\n","    \"\"\"\n","    for f in args:\n","        if f.has(sp.nan) or f.has(sp.oo) or f.has(-sp.oo) or f.has(sp.zoo):\n","            return True\n","    return False\n","\n","\n","class CharSPEnvironment(object):\n","\n","    # https://docs.sympy.org/latest/modules/functions/elementary.html#real-root\n","\n","    SYMPY_OPERATORS = {\n","        # Elementary functions\n","        sp.Add: 'add',\n","        sp.Mul: 'mul',\n","        sp.Pow: 'pow',\n","        sp.exp: 'exp',\n","        sp.log: 'ln',\n","        sp.Abs: 'abs',\n","        sp.sign: 'sign',\n","        # Trigonometric Functions\n","        sp.sin: 'sin',\n","        sp.cos: 'cos',\n","        sp.tan: 'tan',\n","        sp.cot: 'cot',\n","        sp.sec: 'sec',\n","        sp.csc: 'csc',\n","        # Trigonometric Inverses\n","        sp.asin: 'asin',\n","        sp.acos: 'acos',\n","        sp.atan: 'atan',\n","        sp.acot: 'acot',\n","        sp.asec: 'asec',\n","        sp.acsc: 'acsc',\n","        # Hyperbolic Functions\n","        sp.sinh: 'sinh',\n","        sp.cosh: 'cosh',\n","        sp.tanh: 'tanh',\n","        sp.coth: 'coth',\n","        sp.sech: 'sech',\n","        sp.csch: 'csch',\n","        # Hyperbolic Inverses\n","        sp.asinh: 'asinh',\n","        sp.acosh: 'acosh',\n","        sp.atanh: 'atanh',\n","        sp.acoth: 'acoth',\n","        sp.asech: 'asech',\n","        sp.acsch: 'acsch',\n","        # Derivative\n","        sp.Derivative: 'derivative',\n","    }\n","\n","    OPERATORS = {\n","        # Elementary functions\n","        'add': 2,\n","        'sub': 2,\n","        'mul': 2,\n","        'div': 2,\n","        'pow': 2,\n","        'rac': 2,\n","        'inv': 1,\n","        'pow2': 1,\n","        'pow3': 1,\n","        'pow4': 1,\n","        'pow5': 1,\n","        'sqrt': 1,\n","        'exp': 1,\n","        'ln': 1,\n","        'abs': 1,\n","        'sign': 1,\n","        # Trigonometric Functions\n","        'sin': 1,\n","        'cos': 1,\n","        'tan': 1,\n","        'cot': 1,\n","        'sec': 1,\n","        'csc': 1,\n","        # Trigonometric Inverses\n","        'asin': 1,\n","        'acos': 1,\n","        'atan': 1,\n","        'acot': 1,\n","        'asec': 1,\n","        'acsc': 1,\n","        # Hyperbolic Functions\n","        'sinh': 1,\n","        'cosh': 1,\n","        'tanh': 1,\n","        'coth': 1,\n","        'sech': 1,\n","        'csch': 1,\n","        # Hyperbolic Inverses\n","        'asinh': 1,\n","        'acosh': 1,\n","        'atanh': 1,\n","        'acoth': 1,\n","        'asech': 1,\n","        'acsch': 1,\n","        # Derivative\n","        'derivative': 2,\n","        # custom functions\n","        'f': 1,\n","        'g': 2,\n","        'h': 3,\n","    }\n","\n","    def __init__(self, params):\n","\n","        self.max_int = params.max_int\n","        self.max_ops = params.max_ops\n","        self.max_ops_G = params.max_ops_G\n","        self.int_base = params.int_base\n","        self.balanced = params.balanced\n","        self.positive = params.positive\n","        self.precision = params.precision\n","        self.n_variables = params.n_variables\n","        self.n_coefficients = params.n_coefficients\n","        self.max_len = params.max_len\n","        self.clean_prefix_expr = params.clean_prefix_expr\n","        assert self.max_int >= 1\n","        assert abs(self.int_base) >= 2\n","        assert self.precision >= 2\n","\n","        # parse operators with their weights\n","        self.operators = sorted(list(self.OPERATORS.keys()))\n","        ops = params.operators.split(',')\n","        ops = sorted([x.split(':') for x in ops])\n","        assert len(ops) >= 1 and all(o in self.OPERATORS for o, _ in ops)\n","        self.all_ops = [o for o, _ in ops]\n","        self.una_ops = [o for o, _ in ops if self.OPERATORS[o] == 1]\n","        self.bin_ops = [o for o, _ in ops if self.OPERATORS[o] == 2]\n","\n","        self.all_ops_probs = np.array([float(w) for _, w in ops]).astype(np.float64)\n","        self.una_ops_probs = np.array([float(w) for o, w in ops if self.OPERATORS[o] == 1]).astype(np.float64)\n","        self.bin_ops_probs = np.array([float(w) for o, w in ops if self.OPERATORS[o] == 2]).astype(np.float64)\n","        self.all_ops_probs = self.all_ops_probs / self.all_ops_probs.sum()\n","        self.una_ops_probs = self.una_ops_probs / self.una_ops_probs.sum()\n","        self.bin_ops_probs = self.bin_ops_probs / self.bin_ops_probs.sum()\n","\n","        assert len(self.all_ops) == len(set(self.all_ops)) >= 1\n","        assert set(self.all_ops).issubset(set(self.operators))\n","        assert len(self.all_ops) == len(self.una_ops) + len(self.bin_ops)\n","\n","        # symbols / elements\n","        self.constants = ['pi', 'E']\n","        self.variables = OrderedDict({\n","            'x': sp.Symbol('x', real=True, nonzero=True),  # , positive=True\n","            'y': sp.Symbol('y', real=True, nonzero=True),  # , positive=True\n","            'z': sp.Symbol('z', real=True, nonzero=True),  # , positive=True\n","            't': sp.Symbol('t', real=True, nonzero=True),  # , positive=True\n","        })\n","        self.coefficients = OrderedDict({\n","            f'a{i}': sp.Symbol(f'a{i}', real=True)\n","            for i in range(10)\n","        })\n","        self.functions = OrderedDict({\n","            'f': sp.Function('f', real=True, nonzero=True),\n","            'g': sp.Function('g', real=True, nonzero=True),\n","            'h': sp.Function('h', real=True, nonzero=True),\n","        })\n","        self.symbols = ['I', 'INT+', 'INT-', 'INT', 'FLOAT', '-', '.', '10^', 'Y', \"Y'\", \"Y''\"]\n","        if self.balanced:\n","            assert self.int_base > 2\n","            max_digit = (self.int_base + 1) // 2\n","            self.elements = [str(i) for i in range(max_digit - abs(self.int_base), max_digit)]\n","        else:\n","            self.elements = [str(i) for i in range(abs(self.int_base))]\n","        assert 1 <= self.n_variables <= len(self.variables)\n","        assert 0 <= self.n_coefficients <= len(self.coefficients)\n","        assert all(k in self.OPERATORS for k in self.functions.keys())\n","        assert all(v in self.OPERATORS for v in self.SYMPY_OPERATORS.values())\n","\n","        # SymPy elements\n","        self.local_dict = {}\n","        for k, v in list(self.variables.items()) + list(self.coefficients.items()) + list(self.functions.items()):\n","            assert k not in self.local_dict\n","            self.local_dict[k] = v\n","\n","        # vocabulary\n","        self.words = SPECIAL_WORDS + self.constants + list(self.variables.keys()) + list(self.coefficients.keys()) + self.operators + self.symbols + self.elements\n","        self.id2word = {i + 1: s for i, s in enumerate(self.words)}\n","        self.word2id = {s: i for i, s in self.id2word.items()}\n","        assert len(self.words) == len(set(self.words))\n","\n","        # number of words / indices\n","        self.n_words = params.n_words = len(self.words)\n","        self.eos_index = params.eos_index = 0\n","        self.pad_index = params.pad_index = 1\n","\n","        # leaf probabilities\n","        s = [float(x) for x in params.leaf_probs.split(',')]\n","        assert len(s) == 4 and all(x >= 0 for x in s)\n","        self.leaf_probs = np.array(s).astype(np.float64)\n","        self.leaf_probs = self.leaf_probs / self.leaf_probs.sum()\n","        assert self.leaf_probs[0] > 0\n","        assert (self.leaf_probs[1] == 0) == (self.n_coefficients == 0)\n","\n","        # possible leaves\n","        self.n_leaves = self.n_variables + self.n_coefficients\n","        if self.leaf_probs[2] > 0:\n","            self.n_leaves += self.max_int * (1 if self.positive else 2)\n","        if self.leaf_probs[3] > 0:\n","            self.n_leaves += len(self.constants)\n","\n","        # generation parameters\n","        self.nl = 1  # self.n_leaves\n","        self.p1 = 1  # len(self.una_ops)\n","        self.p2 = 1  # len(self.bin_ops)\n","\n","        # initialize distribution for binary and unary-binary trees\n","        self.bin_dist = self.generate_bin_dist(params.max_ops)\n","        self.ubi_dist = self.generate_ubi_dist(params.max_ops)\n","\n","        # rewrite expressions\n","        self.rewrite_functions = [x for x in params.rewrite_functions.split(',') if x != '']\n","        assert len(self.rewrite_functions) == len(set(self.rewrite_functions))\n","        assert all(x in ['expand', 'factor', 'expand_log', 'logcombine', 'powsimp', 'simplify'] for x in self.rewrite_functions)\n","\n","    def generate_bin_dist(self, max_ops):\n","        \"\"\"\n","        `max_ops`: maximum number of operators\n","        Enumerate the number of possible binary trees that can be generated from empty nodes.\n","        D[e][n] represents the number of different binary trees with n nodes that\n","        can be generated from e empty nodes, using the following recursion:\n","            D(0, n) = 0\n","            D(1, n) = C_n (n-th Catalan number)\n","            D(e, n) = D(e - 1, n + 1) - D(e - 2, n + 1)\n","        \"\"\"\n","        # initialize Catalan numbers\n","        catalans = [1]\n","        for i in range(1, 2 * max_ops + 1):\n","            catalans.append((4 * i - 2) * catalans[i - 1] // (i + 1))\n","\n","        # enumerate possible trees\n","        D = []\n","        for e in range(max_ops + 1):  # number of empty nodes\n","            s = []\n","            for n in range(2 * max_ops - e + 1):  # number of operators\n","                if e == 0:\n","                    s.append(0)\n","                elif e == 1:\n","                    s.append(catalans[n])\n","                else:\n","                    s.append(D[e - 1][n + 1] - D[e - 2][n + 1])\n","            D.append(s)\n","        return D\n","\n","    def generate_ubi_dist(self, max_ops):\n","        \"\"\"\n","        `max_ops`: maximum number of operators\n","        Enumerate the number of possible unary-binary trees that can be generated from empty nodes.\n","        D[e][n] represents the number of different binary trees with n nodes that\n","        can be generated from e empty nodes, using the following recursion:\n","            D(0, n) = 0\n","            D(e, 0) = L ** e\n","            D(e, n) = L * D(e - 1, n) + p_1 * D(e, n - 1) + p_2 * D(e + 1, n - 1)\n","        \"\"\"\n","        # enumerate possible trees\n","        # first generate the tranposed version of D, then transpose it\n","        D = []\n","        D.append([0] + ([self.nl ** i for i in range(1, 2 * max_ops + 1)]))\n","        for n in range(1, 2 * max_ops + 1):  # number of operators\n","            s = [0]\n","            for e in range(1, 2 * max_ops - n + 1):  # number of empty nodes\n","                s.append(self.nl * s[e - 1] + self.p1 * D[n - 1][e] + self.p2 * D[n - 1][e + 1])\n","            D.append(s)\n","        assert all(len(D[i]) >= len(D[i + 1]) for i in range(len(D) - 1))\n","        D = [[D[j][i] for j in range(len(D)) if i < len(D[j])] for i in range(max(len(x) for x in D))]\n","        return D\n","\n","    def write_int(self, val):\n","        \"\"\"\n","        Convert a decimal integer to a representation in the given base.\n","        The base can be negative.\n","        In balanced bases (positive), digits range from -(base-1)//2 to (base-1)//2\n","        \"\"\"\n","        base = self.int_base\n","        balanced = self.balanced\n","        res = []\n","        max_digit = abs(base)\n","        if balanced:\n","            max_digit = (base - 1) // 2\n","        else:\n","            if base > 0:\n","                neg = val < 0\n","                val = -val if neg else val\n","        while True:\n","            rem = val % base\n","            val = val // base\n","            if rem < 0 or rem > max_digit:\n","                rem -= base\n","                val += 1\n","            res.append(str(rem))\n","            if val == 0:\n","                break\n","        if base < 0 or balanced:\n","            res.append('INT')\n","        else:\n","            res.append('INT-' if neg else 'INT+')\n","        return res[::-1]\n","\n","    def parse_int(self, lst):\n","        \"\"\"\n","        Parse a list that starts with an integer.\n","        Return the integer value, and the position it ends in the list.\n","        \"\"\"\n","        base = self.int_base\n","        balanced = self.balanced\n","        val = 0\n","        if not (balanced and lst[0] == 'INT' or base >= 2 and lst[0] in ['INT+', 'INT-'] or base <= -2 and lst[0] == 'INT'):\n","            raise InvalidPrefixExpression(f\"Invalid integer in prefix expression\")\n","        i = 0\n","        for x in lst[1:]:\n","            if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n","                break\n","            val = val * base + int(x)\n","            i += 1\n","        if base > 0 and lst[0] == 'INT-':\n","            val = -val\n","        return val, i + 1\n","\n","    def sample_next_pos_ubi(self, nb_empty, nb_ops, rng):\n","        \"\"\"\n","        Sample the position of the next node (unary-binary case).\n","        Sample a position in {0, ..., `nb_empty` - 1}, along with an arity.\n","        \"\"\"\n","        assert nb_empty > 0\n","        assert nb_ops > 0\n","        probs = []\n","        for i in range(nb_empty):\n","            probs.append((self.nl ** i) * self.p1 * self.ubi_dist[nb_empty - i][nb_ops - 1])\n","        for i in range(nb_empty):\n","            probs.append((self.nl ** i) * self.p2 * self.ubi_dist[nb_empty - i + 1][nb_ops - 1])\n","        probs = [p / self.ubi_dist[nb_empty][nb_ops] for p in probs]\n","        probs = np.array(probs, dtype=np.float64)\n","        e = rng.choice(2 * nb_empty, p=probs)\n","        arity = 1 if e < nb_empty else 2\n","        e = e % nb_empty\n","        return e, arity\n","\n","    def get_leaf(self, max_int, rng):\n","        \"\"\"\n","        Generate a leaf.\n","        \"\"\"\n","        self.leaf_probs\n","        leaf_type = rng.choice(4, p=self.leaf_probs)\n","        if leaf_type == 0:\n","            return [list(self.variables.keys())[rng.randint(self.n_variables)]]\n","        elif leaf_type == 1:\n","            return [list(self.coefficients.keys())[rng.randint(self.n_coefficients)]]\n","        elif leaf_type == 2:\n","            c = rng.randint(1, max_int + 1)\n","            c = c if (self.positive or rng.randint(2) == 0) else -c\n","            return self.write_int(c)\n","        else:\n","            return [self.constants[rng.randint(len(self.constants))]]\n","\n","    def _generate_expr(self, nb_total_ops, max_int, rng, require_x=False, require_y=False, require_z=False):\n","        \"\"\"\n","        Create a tree with exactly `nb_total_ops` operators.\n","        \"\"\"\n","        stack = [None]\n","        nb_empty = 1  # number of empty nodes\n","        l_leaves = 0  # left leaves - None states reserved for leaves\n","        t_leaves = 1  # total number of leaves (just used for sanity check)\n","\n","        # create tree\n","        for nb_ops in range(nb_total_ops, 0, -1):\n","\n","            # next operator, arity and position\n","            skipped, arity = self.sample_next_pos_ubi(nb_empty, nb_ops, rng)\n","            if arity == 1:\n","                op = rng.choice(self.una_ops, p=self.una_ops_probs)\n","            else:\n","                op = rng.choice(self.bin_ops, p=self.bin_ops_probs)\n","\n","            nb_empty += self.OPERATORS[op] - 1 - skipped  # created empty nodes - skipped future leaves\n","            t_leaves += self.OPERATORS[op] - 1            # update number of total leaves\n","            l_leaves += skipped                           # update number of left leaves\n","\n","            # update tree\n","            pos = [i for i, v in enumerate(stack) if v is None][l_leaves]\n","            stack = stack[:pos] + [op] + [None for _ in range(self.OPERATORS[op])] + stack[pos + 1:]\n","\n","        # sanity check\n","        assert len([1 for v in stack if v in self.all_ops]) == nb_total_ops\n","        assert len([1 for v in stack if v is None]) == t_leaves\n","\n","        # create leaves\n","        # optionally add variables x, y, z if possible\n","        assert not require_z or require_y\n","        assert not require_y or require_x\n","        leaves = [self.get_leaf(max_int, rng) for _ in range(t_leaves)]\n","        if require_z and t_leaves >= 2:\n","            leaves[1] = ['z']\n","        if require_y:\n","            leaves[0] = ['y']\n","        if require_x and not any(len(leaf) == 1 and leaf[0] == 'x' for leaf in leaves):\n","            leaves[-1] = ['x']\n","        rng.shuffle(leaves)\n","\n","        # insert leaves into tree\n","        for pos in range(len(stack) - 1, -1, -1):\n","            if stack[pos] is None:\n","                stack = stack[:pos] + leaves.pop() + stack[pos + 1:]\n","\n","        return stack\n","\n","    def write_infix(self, token, args):\n","        \"\"\"\n","        Infix representation.\n","        Convert prefix expressions to a format that SymPy can parse.\n","        \"\"\"\n","        if token == 'add':\n","            return f'({args[0]})+({args[1]})'\n","        elif token == 'sub':\n","            return f'({args[0]})-({args[1]})'\n","        elif token == 'mul':\n","            return f'({args[0]})*({args[1]})'\n","        elif token == 'div':\n","            return f'({args[0]})/({args[1]})'\n","        elif token == 'pow':\n","            return f'({args[0]})**({args[1]})'\n","        elif token == 'rac':\n","            return f'({args[0]})**(1/({args[1]}))'\n","        elif token == 'abs':\n","            return f'Abs({args[0]})'\n","        elif token == 'inv':\n","            return f'1/({args[0]})'\n","        elif token == 'pow2':\n","            return f'({args[0]})**2'\n","        elif token == 'pow3':\n","            return f'({args[0]})**3'\n","        elif token == 'pow4':\n","            return f'({args[0]})**4'\n","        elif token == 'pow5':\n","            return f'({args[0]})**5'\n","        elif token in ['sign', 'sqrt', 'exp', 'ln', 'sin', 'cos', 'tan', 'cot', 'sec', 'csc', 'asin', 'acos', 'atan', 'acot', 'asec', 'acsc', 'sinh', 'cosh', 'tanh', 'coth', 'sech', 'csch', 'asinh', 'acosh', 'atanh', 'acoth', 'asech', 'acsch']:\n","            return f'{token}({args[0]})'\n","        elif token == 'derivative':\n","            return f'Derivative({args[0]},{args[1]})'\n","        elif token == 'f':\n","            return f'f({args[0]})'\n","        elif token == 'g':\n","            return f'g({args[0]},{args[1]})'\n","        elif token == 'h':\n","            return f'h({args[0]},{args[1]},{args[2]})'\n","        elif token.startswith('INT'):\n","            return f'{token[-1]}{args[0]}'\n","        else:\n","            return token\n","        raise InvalidPrefixExpression(f\"Unknown token in prefix expression: {token}, with arguments {args}\")\n","\n","    def _prefix_to_infix(self, expr):\n","        \"\"\"\n","        Parse an expression in prefix mode, and output it in either:\n","          - infix mode (returns human readable string)\n","          - develop mode (returns a dictionary with the simplified expression)\n","        \"\"\"\n","        if len(expr) == 0:\n","            raise InvalidPrefixExpression(\"Empty prefix list.\")\n","        t = expr[0]\n","        if t in self.operators:\n","            args = []\n","            l1 = expr[1:]\n","            for _ in range(self.OPERATORS[t]):\n","                i1, l1 = self._prefix_to_infix(l1)\n","                args.append(i1)\n","            return self.write_infix(t, args), l1\n","        elif t in self.variables or t in self.coefficients or t in self.constants or t == 'I':\n","            return t, expr[1:]\n","        else:\n","            val, i = self.parse_int(expr)\n","            return str(val), expr[i:]\n","\n","    def prefix_to_infix(self, expr):\n","        \"\"\"\n","        Prefix to infix conversion.\n","        \"\"\"\n","        p, r = self._prefix_to_infix(expr)\n","        if len(r) > 0:\n","            raise InvalidPrefixExpression(f\"Incorrect prefix expression \\\"{expr}\\\". \\\"{r}\\\" was not parsed.\")\n","        return f'({p})'\n","\n","    def rewrite_sympy_expr(self, expr):\n","        \"\"\"\n","        Rewrite a SymPy expression.\n","        \"\"\"\n","        expr_rw = expr\n","        for f in self.rewrite_functions:\n","            if f == 'expand':\n","                expr_rw = sp.expand(expr_rw)\n","            elif f == 'factor':\n","                expr_rw = sp.factor(expr_rw)\n","            elif f == 'expand_log':\n","                expr_rw = sp.expand_log(expr_rw, force=True)\n","            elif f == 'logcombine':\n","                expr_rw = sp.logcombine(expr_rw, force=True)\n","            elif f == 'powsimp':\n","                expr_rw = sp.powsimp(expr_rw, force=True)\n","            elif f == 'simplify':\n","                expr_rw = simplify(expr_rw, seconds=1)\n","        return expr_rw\n","\n","    def infix_to_sympy(self, infix, no_rewrite=False, \n","                       evaluate=False, force=False):\n","        \"\"\"\n","        Convert an infix expression to SymPy.\n","        \"\"\"\n","        if force:\n","          return parse_expr(infix, evaluate=evaluate, local_dict=self.local_dict)\n","\n","        if not is_valid_expr(infix):\n","            raise ValueErrorExpression\n","        #expr = parse_expr(infix, evaluate=True, local_dict=self.local_dict)\n","        expr = parse_expr(infix, evaluate=evaluate, local_dict=self.local_dict)\n","\n","        if expr.has(sp.I) or expr.has(AccumBounds):\n","            raise ValueErrorExpression\n","        if not no_rewrite:\n","            expr = self.rewrite_sympy_expr(expr)\n","        return expr\n","\n","    def _sympy_to_prefix(self, op, expr):\n","        \"\"\"\n","        Parse a SymPy expression given an initial root operator.\n","        \"\"\"\n","        n_args = len(expr.args)\n","\n","        # derivative operator\n","        if op == 'derivative':\n","            assert n_args >= 2\n","            assert all(len(arg) == 2 and str(arg[0]) in self.variables and int(arg[1]) >= 1 for arg in expr.args[1:]), expr.args\n","            parse_list = self.sympy_to_prefix(expr.args[0])\n","            for var, degree in expr.args[1:]:\n","                parse_list = ['derivative' for _ in range(int(degree))] + parse_list + [str(var) for _ in range(int(degree))]\n","            return parse_list\n","\n","        assert (op == 'add' or op == 'mul') and (n_args >= 2) or (op != 'add' and op != 'mul') and (1 <= n_args <= 2)\n","\n","        # square root\n","        if op == 'pow' and isinstance(expr.args[1], sp.Rational) and expr.args[1].p == 1 and expr.args[1].q == 2:\n","            return ['sqrt'] + self.sympy_to_prefix(expr.args[0])\n","\n","        # parse children\n","        parse_list = []\n","        for i in range(n_args):\n","            if i == 0 or i < n_args - 1:\n","                parse_list.append(op)\n","            parse_list += self.sympy_to_prefix(expr.args[i])\n","\n","        return parse_list\n","\n","    def sympy_to_prefix(self, expr):\n","        \"\"\"\n","        Convert a SymPy expression to a prefix one.\n","        \"\"\"\n","        if isinstance(expr, sp.Symbol):\n","            return [str(expr)]\n","        elif isinstance(expr, sp.Integer):\n","            return self.write_int(int(str(expr)))\n","        elif isinstance(expr, sp.Rational):\n","            return ['div'] + self.write_int(int(expr.p)) + self.write_int(int(expr.q))\n","        elif expr == sp.E:\n","            return ['E']\n","        elif expr == sp.pi:\n","            return ['pi']\n","        elif expr == sp.I:\n","            return ['I']\n","        # SymPy operator\n","        for op_type, op_name in self.SYMPY_OPERATORS.items():\n","            if isinstance(expr, op_type):\n","                return self._sympy_to_prefix(op_name, expr)\n","        # environment function\n","        for func_name, func in self.functions.items():\n","            if isinstance(expr, func):\n","                return self._sympy_to_prefix(func_name, expr)\n","        # unknown operator\n","        raise UnknownSymPyOperator(f\"Unknown SymPy operator: {expr}\")\n","\n","    def extract_non_constant_subtree(self, expr):\n","        return extract_non_constant_subtree(expr, self.variables.values())\n","\n","    def gen_expr(self, rng):\n","        \"\"\"\n","        Generate pairs of (function, derivative) or (function, primitive).\n","        Start by generating a random function f, and use SymPy to compute f'.\n","        \"\"\"\n","        x = self.variables['x']\n","        if rng.randint(40) == 0:\n","            nb_ops = rng.randint(0, 4)\n","        else:\n","            nb_ops = rng.randint(4, self.max_ops + 1)\n","\n","        try:\n","          # generate an expression and rewrite it,\n","          # avoid issues in 0 and convert to SymPy\n","          F_expr = self._generate_expr(nb_ops, self.max_int, rng)\n","          infix = self.prefix_to_infix(F_expr)\n","          F = self.infix_to_sympy(infix, evaluate=False)\n","\n","          # skip constant expressions\n","          if x not in F.free_symbols:\n","              return None\n","\n","          # remove additive constant, re-index coefficients\n","          f = sp.simplify(F)\n","\n","          # skip invalid expressions\n","          if has_inf_nan(f, F):\n","              return None\n","\n","          # convert back to prefix\n","          f_prefix = self.sympy_to_prefix(f)\n","          F1_prefix = self.sympy_to_prefix(F)\n","          F2_prefix = self.sympy_to_prefix(sp.expand(f))\n","          F3_prefix = self.sympy_to_prefix(sp.expand(F))\n","\n","          return f_prefix, (F1_prefix, F2_prefix, F3_prefix)\n","\n","        except RuntimeWarning:\n","          return None\n","        except TimeoutError:\n","            return None\n","        except (ValueErrorExpression, UnknownSymPyOperator, OverflowError, TypeError):\n","            return None\n","        except Exception as e:\n","            return None"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgJZQvMcoFKi","executionInfo":{"status":"ok","timestamp":1616542611978,"user_tz":240,"elapsed":4104,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["DATASET_ENV = CharSPEnvironment(params)\n","\n","\n","def sympy_to_sequences(expr, env):\n","  prefix = env.sympy_to_prefix(expr)\n","  return [env.word2id[w] for w in prefix]\n","\n","\n","def sequences_to_sympy(x, env):\n","  x = [env.id2word[w] for w in x]\n","  x = env.prefix_to_infix(x)\n","  return env.infix_to_sympy(x)\n","\n","\n","def prefix_to_sympy(x, env):\n","  x = env.prefix_to_infix(x)\n","  return env.infix_to_sympy(x)"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JniEgiR5hRKy"},"source":["## Visualising some random expressions\n","\n","The the function 'DATASET_ENV.gen_expr' generates random equations in prefix notation (see https://en.wikipedia.org/wiki/Polish_notation for more details)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":39},"id":"jfNCAbpOwnG3","executionInfo":{"status":"ok","timestamp":1616540070164,"user_tz":240,"elapsed":282,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"940d8396-95d6-4335-b1ea-8947799fd3d2"},"source":["irreducible_eq, reducible_eqs = DATASET_ENV.gen_expr(np.random)\n","\n","prefix_to_sympy(irreducible_eq, DATASET_ENV)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle x y + \\left(x + 4\\right)^{2}$","text/plain":["x*y + (x + 4)**2"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"G490TLqkFQi-","colab":{"base_uri":"https://localhost:8080/","height":39},"executionInfo":{"status":"ok","timestamp":1616540080162,"user_tz":240,"elapsed":249,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"11826852-2e71-456b-f7de-99735818254e"},"source":["prefix_to_sympy(reducible_eqs[0], DATASET_ENV)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle x y + \\left(x + 4\\right)^{2}$","text/plain":["x*y + (x + 4)**2"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"cmrUse1KqVTy","executionInfo":{"status":"ok","timestamp":1616540087973,"user_tz":240,"elapsed":249,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"06ede0de-5f02-46e9-97b8-c993c0e0034c"},"source":["prefix_to_sympy(reducible_eqs[1], DATASET_ENV)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle x^{2} + x y + 8 x + 16$","text/plain":["x**2 + x*y + 8*x + 16"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"ZM-W4-maqWXR","executionInfo":{"status":"ok","timestamp":1616540092939,"user_tz":240,"elapsed":199,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"928130fa-528a-4a24-8ce6-a4a76db38f08"},"source":["prefix_to_sympy(reducible_eqs[2], DATASET_ENV)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle x^{2} + x y + 8 x + 16$","text/plain":["x**2 + x*y + 8*x + 16"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"sITcQfTLs9FU"},"source":["# Dataset creation\n","\n","Here we use python's multiprocessing module to parallelise dataset createtion. \\\\\n","The following creates two files of the tokenize equations 'x.pkl' and 'y.pkl'. Where 'y.pkl' contains the simplified versions of the equations found in 'x.pkl'.\n"]},{"cell_type":"code","metadata":{"id":"PfBUcui1okcb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"63857eb6-423f-48a4-8703-6e28f945f596"},"source":["from multiprocessing import Process, Manager\n","from IPython import display\n","import pickle\n","\n","\n","_samples_per_process = int(params.dataset_size//params.num_parallel_processes)\n","\n","\n","def sampling_func(inp, tar):\n","  np.random.seed(None)\n","  i = 0\n","  while i < _samples_per_process:\n","    x = None\n","    while x is None:\n","      x = DATASET_ENV.gen_expr(np.random)\n","    y, x = x\n","    y = [DATASET_ENV.word2id[w] for w in y]\n","    i += 1\n","    for u in x:\n","      if len(u) > 1:\n","        inp.append([DATASET_ENV.word2id[w] for w in u])\n","        tar.append(y)\n","\n","\n","print('Creating dataset ...')\n","start = time.time()\n","\n","manager = Manager()\n","inp = manager.list() \n","tar = manager.list() \n","\n","processes = []\n","for _ in range(params.num_parallel_processes):\n","  p = Process(target=sampling_func, args=(inp, tar)) \n","  p.start()\n","  processes.append(p)\n","for p in processes:\n","  p.join()\n","\n","inp = list(inp)\n","tar = list(tar)\n","\n","display.clear_output()\n","\n","print('Saving')\n","with open(params.dataset_path + 'x.pkl', 'wb') as f:\n","  pickle.dump(inp, f)\n","with open(params.dataset_path + 'y.pkl', 'wb') as f:\n","  pickle.dump(tar, f)\n","\n","clear_cache()\n","manager.shutdown()\n","print('Complete')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating dataset ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OLhr_lWpJSJ9"},"source":["## Model Setup\n","This cell defines the keras architecture of the standard transformer. \n","Here we use a variant of the original transformer architecture known as ReZero (see [arXiv:2003.04887](https://arxiv.org/abs/2003.04887) ) for faster training convergence. "]},{"cell_type":"code","metadata":{"id":"N1CZTZI_JUh0","executionInfo":{"status":"ok","timestamp":1616536439862,"user_tz":240,"elapsed":922,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","\n","def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)\n","\n","\n","def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by \n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","  return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","\n","def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates\n","\n","\n","def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","\n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights\n","\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","\n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","    return output, attention_weights\n","\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])\n","\n","\n","class ReZero(tf.keras.layers.Layer):\n","    def __init__(self, name):\n","        super(ReZero, self).__init__(name=name)\n","        a_init = tf.zeros_initializer()\n","        self.alpha = tf.Variable(name=self.name + '-alpha',\n","            initial_value=a_init(shape=(1,), dtype=\"float32\"), trainable=True\n","        )\n","\n","    def call(self, inputs):\n","        return self.alpha * inputs\n","\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    #self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    #self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.rz1 = ReZero(self.name + 'rz1')\n","    self.rz2 = ReZero(self.name + 'rz2')\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = x + self.rz1(attn_output)\n","    #out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = out1 + self.rz2(ffn_output)\n","    #out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    return out2\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    #self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    #self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    #self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.rz1 = ReZero(self.name + 'rz1')\n","    self.rz2 = ReZero(self.name + 'rz2')\n","    self.rz3 = ReZero(self.name + 'rz3')\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","\n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = x + self.rz1(attn1)\n","    #out1 = self.layernorm1(attn1 + x)\n","\n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = out1 + self.rz2(attn2)\n","    #out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = out2 + self.rz3(ffn_output)\n","    #out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","    return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","\n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","\n","    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x  # (batch_size, input_seq_len, d_model)\n","\n","\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff,\n","               maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","\n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights\n","\n","\n","class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.tok_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.dec_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","\n","    self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n","                             pe_input, rate)\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n","                           pe_target, rate)\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","    \n","    inp = self.tok_embedding(inp)\n","    enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    tar = self.dec_embedding(tar)\n","    dec_output, attention_weights = self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","    return final_output, attention_weights"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rpVylKELT6d"},"source":["# Load the dataset\n","Load the datasets of equations. You can create your own or load a pre-computed one from here and load it into the "]},{"cell_type":"code","metadata":{"id":"NplJT2z7Fb3q","executionInfo":{"status":"ok","timestamp":1616540114202,"user_tz":240,"elapsed":208,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["SYMBOL_VOCAB_SIZE = DATASET_ENV.n_words\n","START_TOKEN = SYMBOL_VOCAB_SIZE + 1\n","END_TOKEN = SYMBOL_VOCAB_SIZE + 2\n","VOCAB_SIZE = END_TOKEN + 1"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"KysLwLnK49N0","executionInfo":{"status":"ok","timestamp":1616540117578,"user_tz":240,"elapsed":818,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["class SimplifyDataset:\n","  def __init__(self, path, inp_data_len=128, tar_data_len=128):\n","    self.path = path\n","    self.inp_data_len = inp_data_len\n","    self.tar_data_len = tar_data_len\n","    self.num_train_examples = None\n","    self.num_test_examples = None\n","\n","  def preprocess_tokens(self, w, maxlen, wrap_start_end=False):\n","    if wrap_start_end:\n","      w = [[START_TOKEN] + x + [END_TOKEN] for x in w]\n","\n","    w = tf.keras.preprocessing.sequence.pad_sequences(w, maxlen=maxlen,\n","                                                      padding='post')\n","    return w\n","\n","  def load_dataset(self, num_examples=None):\n","    with open(self.path + 'x.pkl', 'rb') as f:\n","      src = pickle.load(f)\n","    with open(self.path + 'y.pkl', 'rb') as f:\n","      tar = pickle.load(f)\n","\n","    if num_examples is not None:\n","      src, tar = src[:num_examples], tar[:num_examples]\n","\n","    return src, tar\n","\n","  def call(self, buffer_size, batch_size, num_examples=None):\n","    input, target = self.load_dataset(num_examples)\n","    input = self.preprocess_tokens(input, self.inp_data_len)\n","    target = self.preprocess_tokens(target, self.tar_data_len, True)\n","\n","    input_train, input_val, target_train, target_val = train_test_split(input, target, test_size=0.1)\n","\n","    self.num_train_examples = len(input_train)\n","    self.num_test_examples = len(input_val)\n","\n","    train_dataset = tf.data.Dataset.from_tensor_slices(\n","        (input_train, target_train))\n","    train_dataset = train_dataset.shuffle(buffer_size)\n","    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n","    train_dataset = train_dataset.repeat()\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices((input_val, target_val))\n","    val_dataset = val_dataset.batch(batch_size, drop_remainder=True).repeat()\n","\n","    return train_dataset, val_dataset\n","\n","\n","dataset = SimplifyDataset(params.dataset_path)\n","train_dataset, val_dataset = dataset.call(32000, params.batch_size)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLJ2hFHuKwUL"},"source":["train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQhHBV0us8wI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616537873850,"user_tz":240,"elapsed":338,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"1d6c40d5-0c28-4867-de76-6f7cfaf3ac9c"},"source":["dataset.num_train_examples, dataset.num_test_examples"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(253, 29)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"2IxURwHrzmqc"},"source":["VOCAB_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hv_PL9VTdoA"},"source":["## Model Setup\n","This cell contains the functions that builds the transformer model in Keras. \n","Here we use a variant of the original transformer architecture known as ReZero (see [arXiv:2003.04887](https://arxiv.org/abs/2003.04887) ) for faster training convergence. "]},{"cell_type":"markdown","metadata":{"id":"QO4Go9zBTrW3"},"source":["## Model Design"]},{"cell_type":"code","metadata":{"id":"xnK6EnNTTdoA","executionInfo":{"status":"ok","timestamp":1616540618675,"user_tz":240,"elapsed":962,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","\n","def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)\n","\n","\n","def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by \n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","  return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","\n","def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates\n","\n","\n","def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","\n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights\n","\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","\n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","    return output, attention_weights\n","\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])\n","\n","\n","class ReZero(tf.keras.layers.Layer):\n","    def __init__(self, name):\n","        super(ReZero, self).__init__(name=name)\n","        a_init = tf.zeros_initializer()\n","        self.alpha = tf.Variable(name=self.name + '-alpha',\n","            initial_value=a_init(shape=(1,), dtype=\"float32\"), trainable=True\n","        )\n","\n","    def call(self, inputs):\n","        return self.alpha * inputs\n","\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.rz1 = ReZero(self.name + 'rz1')\n","    self.rz2 = ReZero(self.name + 'rz2')\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = x + self.rz1(attn_output)\n","    #out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = out1 + self.rz2(ffn_output)\n","    #out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    return out2\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.rz1 = ReZero(self.name + 'rz1')\n","    self.rz2 = ReZero(self.name + 'rz2')\n","    self.rz3 = ReZero(self.name + 'rz3')\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","\n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = x + self.rz1(attn1)\n","\n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = out1 + self.rz2(attn2)\n","\n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = out2 + self.rz3(ffn_output)\n","\n","    return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","\n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x  # (batch_size, input_seq_len, d_model)\n","\n","\n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff,\n","               maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","\n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights\n","\n","\n","class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.tok_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.dec_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","\n","    self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n","                             pe_input, rate)\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n","                           pe_target, rate)\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","    \n","    inp = self.tok_embedding(inp)\n","    enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    tar = self.dec_embedding(tar)\n","    dec_output, attention_weights = self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","    return final_output, attention_weights"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxGLea0lTxmC"},"source":["## Model Creation"]},{"cell_type":"code","metadata":{"id":"LWDydUDDKj_u","executionInfo":{"status":"ok","timestamp":1616540297030,"user_tz":240,"elapsed":351,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["K.clear_session()\n","\n","transformer = Transformer(\n","    num_layers=params.num_layers,\n","    d_model=params.d_model,\n","    num_heads=params.num_heads,\n","    dff=params.dff,\n","    input_vocab_size=VOCAB_SIZE,\n","    target_vocab_size=VOCAB_SIZE, \n","    pe_input=params.max_len,\n","    pe_target=params.max_len,\n","    rate=params.dropout_rate)\n","\n","\n","learning_rate = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","\n","def accuracy_function(real, pred):\n","  accuracies = tf.equal(tf.cast(real, tf.int64), tf.argmax(pred, axis=2))\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  accuracies = tf.math.logical_and(mask, accuracies)\n","\n","  accuracies = tf.cast(accuracies, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n","\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\n","\n","\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, dataset.inp_data_len), dtype=tf.int32),\n","    tf.TensorSpec(shape=(None, dataset.tar_data_len), dtype=tf.int32),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(accuracy_function(tar_real, predictions))\n","\n","\n","@tf.function(input_signature=train_step_signature)\n","def val_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","  predictions, _ = transformer(inp, tar_inp, \n","                                True, \n","                                enc_padding_mask, \n","                                combined_mask, \n","                                dec_padding_mask)\n","  loss = loss_function(tar_real, predictions)\n","\n","  val_loss(loss)\n","  val_accuracy(accuracy_function(tar_real, predictions))"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mOEbJQQ5xw5"},"source":["## Training Loop"]},{"cell_type":"markdown","metadata":{"id":"XZciFwKpGn_T"},"source":["Automatically saves the model weights that gives the smallest loss on the validation set."]},{"cell_type":"code","metadata":{"id":"xm88ONGPSbcr","colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"status":"error","timestamp":1616540507004,"user_tz":240,"elapsed":15981,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"a7c652fc-91b3-49b5-d2e3-248bcf630dd6"},"source":["steps_per_epoch = dataset.num_train_examples//params.batch_size\n","steps_per_val = dataset.num_test_examples//params.batch_size\n","best_val = np.inf\n","\n","\n","for epoch in range(params.epochs):\n","  start = time.time()\n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  print(f'Ep: {epoch}, Current Best: {best_val}')\n","  for (i, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","    train_step(inp, targ)\n","\n","    if i % 200 == 0:\n","      print('Iter {} Loss {:.4f} Train Acc {:.4f}'.format(\n","            i, train_loss.result(), train_accuracy.result()))\n","      \n","  print('Epoch {:.4f} Train Loss {:.4f} Train Acc {:.4f}'.format(\n","      epoch, train_loss.result(), train_accuracy.result()))\n","  print('Time taken for 1 epoch: {:.4f} secs'.format(time.time() - start))\n","\n","  val_loss.reset_states()\n","  val_accuracy.reset_states()\n","  for inp, targ in train_dataset.take(steps_per_val):\n","    val_step(inp, targ)\n","  validation_loss = val_loss.result()\n","  validation_acc = val_accuracy.result()\n","\n","  display.clear_output()\n","  print(\"Validation Loss: {:.4f} Vaidation Acc {:.4f}\\n\".format(\n","      validation_loss, validation_acc))\n","  if validation_loss < best_val:\n","    transformer.save_weights(params.model_path)\n","    best_val = validation_loss\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Validation Loss: 0.0000 Vaidation Acc 0.0000\n","\n","Ep: 15, Current Best: 0.0\n","Iter 0 Loss 1.9330 Train Acc 0.3293\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-cf1d2ec9784c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Ep: {epoch}, Current Best: {best_val}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dUL7_bUcQP8S"},"source":["# Evaluate\n","\n","Here we test our model on select equations. Obviously, quality of the output varies depending on dataset and model size. Here an 'InvalidPrefixExpression' is thrown when the model outputs an invalid expression."]},{"cell_type":"code","metadata":{"id":"ZGLaPAulCAfr","executionInfo":{"status":"ok","timestamp":1616540468722,"user_tz":240,"elapsed":200,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}}},"source":["def evaluate(eqa, max_length=40):\n","  # inp sentence is portuguese, hence adding the start and end token\n","  #eqa = DATASET_ENV.sympy_to_prefix(eqa)\n","  #eqa = [DATASET_ENV.word2id[w] for w in eqa]\n","  #eqa = np.array(eqa)\n","  eqa = tf.convert_to_tensor([eqa])\n","  #sentence = tokenizers.pt.tokenize(sentence).to_tensor()\n","\n","  encoder_input = eqa\n","\n","  # as the target is english, the first word to the transformer should be the\n","  # english start token.\n","  output = tf.convert_to_tensor([START_TOKEN])\n","  output = tf.expand_dims(output, 0)\n","\n","  for i in range(max_length):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","\n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","\n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.argmax(predictions, axis=-1)\n","    predicted_id = tf.cast(predicted_id, tf.int32)\n","\n","    # return the result if the predicted_id is equal to the end token\n","    if predicted_id == END_TOKEN:\n","      break\n","\n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return output[:, 1:]"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"AGOdkcKYM5QO","executionInfo":{"status":"ok","timestamp":1616540470880,"user_tz":240,"elapsed":206,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"febff08f-20d0-4419-c1ce-9fe7a9213a59"},"source":["from sympy.parsing.sympy_parser import (parse_expr, standard_transformations, \n","                                        implicit_multiplication_application,\n","                                        convert_xor)\n","from sympy import symbols, diff, init_printing, preorder_traversal, simplify\n","\n","test_expr = \"2x - 2x\"\n","TRANSFORMATIONS = (standard_transformations + (implicit_multiplication_application,) + (convert_xor,))\n","test_expr = parse_expr(test_expr, transformations=TRANSFORMATIONS, evaluate=False)\n","test_expr"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle - 2 x + 2 x$","text/plain":["-2*x + 2*x"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"E9VNa7hwQz-T"},"source":["model output:"]},{"cell_type":"code","metadata":{"id":"PfI_5WW-aAzg","colab":{"base_uri":"https://localhost:8080/","height":344},"executionInfo":{"status":"error","timestamp":1616540517804,"user_tz":240,"elapsed":1284,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"032e5477-44db-410a-ca58-cac18348dd24"},"source":["model_output = evaluate(sympy_to_sequences(test_expr, DATASET_ENV)).numpy()[0]\n","sequences_to_sympy(model_output, DATASET_ENV)"],"execution_count":39,"outputs":[{"output_type":"error","ename":"InvalidPrefixExpression","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidPrefixExpression\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-f08637f56cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_ENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequences_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_ENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-f66e3e530879>\u001b[0m in \u001b[0;36msequences_to_sympy\u001b[0;34m(x, env)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfix_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36mprefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mPrefix\u001b[0m \u001b[0mto\u001b[0m \u001b[0minfix\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \"\"\"\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidPrefixExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incorrect prefix expression \\\"{expr}\\\". \\\"{r}\\\" was not parsed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36m_prefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPERATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36m_prefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPERATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36m_prefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPERATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36m_prefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPERATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-fb42ff71c5a1>\u001b[0m in \u001b[0;36m_prefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidPrefixExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty prefix list.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidPrefixExpression\u001b[0m: 'Empty prefix list.'"]}]},{"cell_type":"markdown","metadata":{"id":"4pHw2zIkQ1s2"},"source":["simplification according to Sympy:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"EqTZ6tGpZmaa","executionInfo":{"status":"ok","timestamp":1616539619986,"user_tz":240,"elapsed":230,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"9b8555c1-54b5-4fa3-f7ac-beb39b01b3e5"},"source":["simplify(test_expr)"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle 0$","text/plain":["0"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"QlM0UehXUS4n","executionInfo":{"status":"ok","timestamp":1616539676844,"user_tz":240,"elapsed":253,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"38cbc6d9-a695-4c4b-9150-096d410e03d0"},"source":["test_expr = \"2x^3+5x^3\"\n","TRANSFORMATIONS = (standard_transformations + (implicit_multiplication_application,) + (convert_xor,))\n","test_expr = parse_expr(test_expr, transformations=TRANSFORMATIONS, evaluate=False)\n","test_expr"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle 2 x^{3} + 5 x^{3}$","text/plain":["2*x**3 + 5*x**3"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"ueP3uj6VRhqL"},"source":["model output:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"id":"YNvx4XayRhqN","executionInfo":{"status":"error","timestamp":1616539771975,"user_tz":240,"elapsed":1727,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"7b522973-b136-4cce-fc62-a27ad6e84e40"},"source":["model_output = evaluate(sympy_to_sequences(test_expr, DATASET_ENV)).numpy()[0]\n","sequences_to_sympy(model_output, DATASET_ENV)"],"execution_count":102,"outputs":[{"output_type":"error","ename":"InvalidPrefixExpression","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidPrefixExpression\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-f08637f56cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_ENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequences_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_ENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-92-1876069c8a91>\u001b[0m in \u001b[0;36msequences_to_sympy\u001b[0;34m(x, env)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfix_to_sympy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-ecaedaaa3feb>\u001b[0m in \u001b[0;36mprefix_to_infix\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefix_to_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidPrefixExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incorrect prefix expression \\\"{expr}\\\". \\\"{r}\\\" was not parsed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'({p})'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidPrefixExpression\u001b[0m: 'Incorrect prefix expression \"[\\'mul\\', \\'INT-\\', \\'1\\', \\'mul\\', \\'x\\', \\'add\\', \\'mul\\', \\'INT-\\', \\'1\\', \\'y\\', \\'INT+\\', \\'3\\', \\'mul\\', \\'INT+\\', \\'2\\', \\'y\\']\". \"[\\'mul\\', \\'INT+\\', \\'2\\', \\'y\\']\" was not parsed.'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"asmG9cJOwyGz","executionInfo":{"status":"ok","timestamp":1616539842763,"user_tz":240,"elapsed":292,"user":{"displayName":"Dean Ninalga","photoUrl":"","userId":"10491677686044089554"}},"outputId":"75d803c5-c797-4ad9-a4b7-7a9642a30386"},"source":["simplify(test_expr)"],"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$\\displaystyle 7 x^{3}$","text/plain":["7*x**3"]},"metadata":{"tags":[]},"execution_count":103}]}]}